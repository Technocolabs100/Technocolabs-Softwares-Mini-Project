import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib
matplotlib.rcParams['figure.figsize'] = (20,10)
import warnings
warnings.filterwarnings('ignore')
IMPORTING DATA
dtest = pd.read_csv('Test.csv')
dtrain = pd.read_csv('Train.csv')
dtrain.head(5)
dtrain.shape
dtrain.describe()
dtrain.info()
CHECKING NULL VALUES
dtest.isnull().sum().sort_values(ascending= False)
dtest.isnull().sum().sort_values(ascending = False)/len(dtest)*100
dtrain.isnull().sum().sort_values(ascending = False)/len(dtrain)*100
Null percentage is 28 in Outlet_Size and 17 in Item_weight .Since the null value percentage is below 45% we can treat these null values 
TREATING THE NULL VALUES 
TEST DATA 
dtest.Outlet_Size.value_counts()
model1 = dtest.Outlet_Size.mode().values[0]
model1
dtest.Outlet_Size =  dtest["Outlet_Size"].replace(np.nan, model1) 
dtest.isnull().sum().sort_values(ascending= False)
Outletsize null values are found 0 
model2 = dtest.Item_Weight.mean()
np.round(model2,3)
dtest["Item_Weight"] = dtest["Item_Weight"].replace(np.nan,model2)
dtest.isnull().sum()
TRAIN DATA 
dtrain.isnull().sum().sort_values(ascending = False)
model3 = dtrain.Outlet_Size.mode().values[0]
model4= dtrain.Item_Weight.mean()
np.round(model4,3)
dtrain.Item_Weight = dtrain.Item_Weight.replace(np.nan, model4)
dtrain.isnull().sum()
DUPLICATE VALUES
duplicate = dtrain.duplicated()
print(duplicate.sum())
dtrain[duplicate]
duplicate= dtest.duplicated()
print(duplicate.sum())
dtest[duplicate]
CHECKING OUTLIERS 
import seaborn as sns
plt.figure(figsize=(8,5))
sns.boxplot(x=dtrain['Item_Weight'])
plt.show()
UNIVARIATE ANALYSIS 
Distribution of the variable Item_Fat_Content
dtrain['Item_Fat_Content'].value_counts()
plt.figure( figsize=(12, 7) )
sns.countplot( dtrain['Item_Fat_Content'] )
plt.show()
dtrain['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = True)
dtest['Item_Fat_Content'].replace(['low fat','LF','reg'],['Low Fat','Low Fat','Regular'],inplace = True)
plt.figure( figsize=(12, 7) )
sns.countplot( dtrain['Item_Fat_Content'] )
plt.show()
Distribution of the variable Item_Type
dtrain['Item_Type'].value_counts()
plt.figure( figsize=(20, 7) )
sns.countplot( dtrain['Item_Type'] )
plt.show()
Distribution of the variable Outlet_Size
dtrain['Outlet_Size'].value_counts()
plt.figure( figsize=(12, 7) )
sns.countplot( dtrain['Outlet_Size'] )
plt.show()
Distribution of the variable Outlet_Location_Type
dtrain['Outlet_Location_Type'].value_counts()
dtrain['Outlet_Location_Type'].value_counts()
dtrain['Outlet_Location_Type'].value_counts()
Distribution of the variable Outlet_Type
dtrain['Outlet_Type'].value_counts()
plt.figure( figsize=(12, 7) )
sns.countplot( dtrain['Outlet_Type'] )
plt.show()
Removing Unwanted columns - since there is no significance of this 
dtrain = dtrain.drop(['Item_Identifier','Outlet_Identifier'], axis = 1)
dtest = dtest.drop(['Item_Identifier','Outlet_Identifier'], axis = 1)
dtrain.head(2)
FINDING CORRELATION BETWEEN DIFFERENT ATTRIBUTES
plt.figure(figsize=(10,10))
sns.heatmap(dtrain.corr(), cmap='Blues',annot = True)
LABEL ENCODER 
from sklearn.preprocessing import LabelEncoder  ## train data
for col in dtrain.columns:
    if dtrain[col].dtype == 'object':
        lbl=LabelEncoder()
        lbl.fit(list(dtrain[col].values))
        dtrain[col]=lbl.transform(dtrain[col].values)
for coll in dtest.columns:     ## test data 
    if dtest[coll].dtype == 'object':
        lbl=LabelEncoder()
        lbl.fit(list(dtest[coll].values))
        dtest[coll]=lbl.transform(dtest[coll].values)  
dtrain.head()
dtest.head()
Spliting data
X_train=dtrain.drop('Item_Outlet_Sales',axis=1)
y_train=dtrain['Item_Outlet_Sales']
X_test= dtest
from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
sc= StandardScaler()
X_train_std= sc.fit_transform(X_train)
X_test_std= sc.transform(X_test)
X_train_std
X_test_std
y_train
LINEAR REGRESSION
from sklearn.linear_model import LinearRegression
lr = LinearRegression(normalize=True)
lr.fit(X_train,y_train)
lr_pred = lr.predict(X_test)
lr_pred
lr_accuracy = round(lr.score(X_train,y_train)*100)
lr_accuracy
Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=400,max_depth=6,min_samples_leaf=100,n_jobs=4)
rf.fit(X_train,y_train)
rf_accuracy = round(rf.score(X_train,y_train)*100)
rf_accuracy
XGBoost
from xgboost import XGBRegressor
model = XGBRegressor(n_estimators = 100, learning_rate=0.05)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_pred
model.score(X_train, y_train)*100
